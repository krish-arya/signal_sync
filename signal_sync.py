# -*- coding: utf-8 -*-
"""signal_sync.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZtw5oKfmY_TRV5LKqkQXiPFLaQ_un46
"""

!pip install torch torchvision torchaudio --quiet
!pip install opencv-python numpy librosa soundfile ffmpeg-python moviepy yolov5 --quiet

!pip install ffmpeg-python librosa torch torchvision torchaudio opencv-python numpy soundfile

from google.colab import drive
drive.mount('/content/drive')

from google.colab.patches import cv2_imshow

!apt-get install -y libgtk2.0-dev pkg-config

import cv2
import torch
import numpy as np
import logging
import tempfile
import ffmpeg
from google.colab.patches import cv2_imshow
from tqdm.notebook import tqdm

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True)
model.conf = 0.4
model.iou = 0.5


vehicle_classes = ["car", "motorbike", "bus", "truck", "bicycle", "ambulance"]


def detect_objects(frame):
    results = model(frame)
    detections = results.xyxy[0].cpu().numpy()
    return detections


def process_video(video_path, output_path):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        logging.error("Error opening video file.")
        return

    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    frame_count = 0

    with tqdm(total=total_frames, desc="Processing Video", unit="frames") as pbar:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            detections = detect_objects(frame)

            for det in detections:
                x_min, y_min, x_max, y_max, conf, class_id = det
                class_id = int(class_id)

                if conf > 0.6 and classes[class_id] in vehicle_classes:
                    label = f"{classes[class_id]} ({conf:.2f})"
                    color = (0, 255, 0) if classes[class_id] != "ambulance" else (0, 0, 255)

                    cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), color, 3)
                    cv2.putText(frame, label, (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

            out.write(frame)

            if frame_count % 10 == 0:
                cv2_imshow(frame)

            frame_count += 1
            pbar.update(1)

    cap.release()
    out.release()
    logging.info(f"âœ… Detection Complete! Output saved at: {output_path}")


video_file = "/content/drive/MyDrive/Aura Cosplay Enva '25 (File responses)/videoplayback (1).mp4"
output_video = "output_with_detections.mp4"


process_video(video_file, output_video)

"""1. Loading the YOLOv8 Model:
The script initializes and loads the YOLOv8 model, which is known for its improved accuracy over previous versions (such as YOLOv5). While the model is trained to detect various objects, the focus here is on detecting vehicles.

2. Video Input and Processing:
The script utilizes OpenCV to read and process the video file frame by frame. This allows the detection system to analyze each individual frame for the presence of vehicles.

3. Object Detection:
Each frame is passed to the YOLOv8 model for object detection. The model identifies objects such as cars, bikes, trucks, and ambulances. If the detection confidence is above a specified threshold (typically 60% or more), a bounding box is drawn around the identified vehicle, and it is labeled accordingly (e.g., "car", "ambulance").

4. Progress Monitoring:
During the video processing, a progress bar is displayed to provide a visual indication of how much of the video has been processed and how many frames remain. This feature is particularly useful for lengthy videos.

5. Output Video:
After processing all frames, the script saves the output video with the drawn bounding boxes and object labels. The final video is saved in the MP4 format, which is widely compatible and efficient for video playback and sharing.

6. Displaying Frames in Google Colab:
Given the use of Google Colab for execution, the script utilizes the cv2_imshow function to display frames. This function is specifically designed for Colab, as cv2.imshow() does not work in this environment. This ensures that the frames are correctly visualized during processing.
"""

import cv2
import torch
import numpy as np
import logging
import librosa
from moviepy.editor import VideoFileClip
from tqdm.notebook import tqdm
from google.colab.patches import cv2_imshow

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Initialize YOLOv5 for car detection
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
model.classes = [2, 5, 7]  # Cars, buses, trucks

def extract_audio(video_path, audio_path):
    with VideoFileClip(video_path) as clip:
        clip.audio.write_audiofile(audio_path, logger=None)

def detect_sirens(audio_path, sr=22050, hop_length=512):
    y, sr = librosa.load(audio_path, sr=sr)
    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)
    zero_crossing = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)
    rms = librosa.feature.rms(y=y, hop_length=hop_length)

    combined = (spectral_centroid / np.max(spectral_centroid)) + \
              (zero_crossing / np.max(zero_crossing)) + \
              (rms / np.max(rms))

    peaks = np.where(combined > np.percentile(combined, 95))[1]
    times = librosa.times_like(combined, sr=sr, hop_length=hop_length)
    return times[peaks]

def process_video(video_path, output_path, audio_path):
    extract_audio(video_path, audio_path)
    siren_times = detect_sirens(audio_path)

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    siren_frames = set()
    for t in siren_times:
        start = int((t - 0.5) * fps)
        end = int((t + 1.5) * fps)
        siren_frames.update(range(start, end))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0
    with tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), desc="Processing") as pbar:
        while True:
            ret, frame = cap.read()
            if not ret: break

            current_time = frame_count / fps

            car_detections = model(frame[..., ::-1])
            for *box, conf, cls in car_detections.xyxy[0]:
                if conf > 0.5:
                    x1, y1, x2, y2 = map(int, box)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, f'Car: {conf:.2f}', (x1, y1-10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)

            if frame_count in siren_frames:
                cv2.rectangle(frame, (0,0), (width, height), (0,0,255), 15)
                cv2.putText(frame, "SIREN DETECTED", (50, 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 4)
                cv2.putText(frame, f"Time: {current_time:.1f}s", (50, 170),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,255), 3)

            out.write(frame)
            if frame_count % int(fps/2) == 0:
                cv2_imshow(cv2.resize(frame, (640, 360)))

            frame_count += 1
            pbar.update(1)

    cap.release()
    out.release()
    logging.info(f"Output saved: {output_path}")

# Usage
video_path = "/content/drive/MyDrive/Aura Cosplay Enva '25 (File responses)/videoplayback (1).mp4"
audio_path = "/content/drive/MyDrive/audio.wav"
output_path = "/content/drive/MyDrive/processed_output.mp4"

process_video(video_path, output_path, audio_path)

import cv2
import torch
import numpy as np
import logging
import librosa
from moviepy.editor import VideoFileClip
from tqdm.notebook import tqdm
from google.colab.patches import cv2_imshow
from scipy.signal import find_peaks

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
model.classes = [2, 5, 7]

def extract_audio(video_path, audio_path):
    with VideoFileClip(video_path) as clip:
        clip.audio.write_audiofile(audio_path, logger=None)

def detect_sirens(audio_path, sr=22050, hop_length=512):
    y, sr = librosa.load(audio_path, sr=sr)
    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)[0]
    rms = librosa.feature.rms(y=y, hop_length=hop_length)[0]
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length)[0]

    centroid_norm = (spectral_centroid - np.mean(spectral_centroid)) / np.std(spectral_centroid)
    rms_norm = (rms - np.mean(rms)) / np.std(rms)
    bandwidth_norm = (spectral_bandwidth - np.mean(spectral_bandwidth)) / np.std(spectral_bandwidth)

    combined = 0.5*centroid_norm + 0.3*rms_norm + 0.2*bandwidth_norm

    peaks, _ = find_peaks(combined,
                         height=np.percentile(combined, 90),
                         distance=int(1.0 * sr/hop_length),
                         width=int(0.5 * sr/hop_length))

    times = librosa.times_like(combined, sr=sr, hop_length=hop_length)
    return times[peaks]

def process_video(video_path, output_path, audio_path):
    extract_audio(video_path, audio_path)
    siren_times = detect_sirens(audio_path)

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    siren_frames = set()
    prev_end = -1
    for t in sorted(siren_times):
        start_frame = int(max(0, (t - 0.3) * fps))
        end_frame = int((t + 0.7) * fps)

        if start_frame > prev_end:
            siren_frames.update(range(start_frame, end_frame))
            prev_end = end_frame
        else:
            siren_frames.update(range(prev_end, end_frame))
            prev_end = end_frame

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0
    with tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), desc="Processing") as pbar:
        while True:
            ret, frame = cap.read()
            if not ret: break

            results = model(frame[..., ::-1])
            car_count = 0
            for *box, conf, cls in results.xyxy[0]:
                if conf > 0.5:
                    x1, y1, x2, y2 = map(int, box)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    car_count += 1

            siren_detected = frame_count in siren_frames
            if siren_detected:
                alpha = 0.3
                overlay = frame.copy()
                cv2.rectangle(overlay, (0,0), (width, height), (0,0,255), -1)
                frame = cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0)
                cv2.putText(frame, "SIREN DETECTED", (50, 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 4)
                cv2.putText(frame, f"Time: {frame_count/fps:.1f}s", (50, 170),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,255), 3)

            cv2.putText(frame, f"Cars: {car_count}", (width-250, 50),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)
            cv2.putText(frame, f"Time: {frame_count/fps:.1f}s", (width-250, 100),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)

            out.write(frame)
            if frame_count % int(fps) == 0:
                cv2_imshow(cv2.resize(frame, (640, 360)))

            frame_count += 1
            pbar.update(1)

    cap.release()
    out.release()
    logging.info(f"Processed {len(siren_times)} siren events. Output: {output_path}")

video_path = "/content/drive/MyDrive/Aura Cosplay Enva '25 (File responses)/videoplayback (1).mp4"
audio_path = "/content/drive/MyDrive/audio.wav"
output_path = "/content/drive/MyDrive/processed_output.mp4"

process_video(video_path, output_path, audio_path)

